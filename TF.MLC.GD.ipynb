{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-04 00:05:10,526 - notMNIST.TF.MLR.training.GD - INFO - Loading pickle file...\n",
      "2018-03-04 00:05:10,575 - notMNIST.TF.MLR.training.GD - DEBUG - Training set (5000, 28, 28) float32 (5000,)\n",
      "2018-03-04 00:05:10,577 - notMNIST.TF.MLR.training.GD - DEBUG - Validation set (1000, 28, 28) float32 (1000,)\n",
      "2018-03-04 00:05:10,578 - notMNIST.TF.MLR.training.GD - DEBUG - Test set (1000, 28, 28) float32 (1000,)\n",
      "2018-03-04 00:05:10,579 - notMNIST.TF.MLR.training.GD - INFO - Pickle file loaded (0.051460 sec).\n",
      "2018-03-04 00:05:10,581 - notMNIST.TF.MLR.training.GD - INFO - Reformating data...\n",
      "2018-03-04 00:05:10,605 - notMNIST.TF.MLR.training.GD - DEBUG - Training set (5000, 784) (5000, 10)\n",
      "2018-03-04 00:05:10,607 - notMNIST.TF.MLR.training.GD - DEBUG - Validation set (1000, 784) (1000, 10)\n",
      "2018-03-04 00:05:10,609 - notMNIST.TF.MLR.training.GD - DEBUG - Test set (1000, 784) (1000, 10)\n",
      "2018-03-04 00:05:10,611 - notMNIST.TF.MLR.training.GD - INFO - data reformatted (0.028655 sec).\n",
      "2018-03-04 00:05:10,612 - notMNIST.TF.MLR.training.GD - INFO - Building computation graph...\n",
      "2018-03-04 00:05:10,731 - notMNIST.TF.MLR.training.GD - INFO - Computation graph built (0.117575 sec).\n",
      "2018-03-04 00:05:10,732 - notMNIST.TF.MLR.training.GD - INFO - Training...\n",
      "2018-03-04 00:05:10,820 - notMNIST.TF.MLR.training.GD - INFO - Initialized\n",
      "2018-03-04 00:05:10,905 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 0: 19.091145\n",
      "2018-03-04 00:05:10,907 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 8.0%\n",
      "2018-03-04 00:05:10,917 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 9.6%\n",
      "2018-03-04 00:05:11,684 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 100: 3.846216\n",
      "2018-03-04 00:05:11,687 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 54.4%\n",
      "2018-03-04 00:05:11,690 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 54.1%\n",
      "2018-03-04 00:05:12,445 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 200: 2.922880\n",
      "2018-03-04 00:05:12,447 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 64.6%\n",
      "2018-03-04 00:05:12,450 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 63.9%\n",
      "2018-03-04 00:05:13,214 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 300: 2.541920\n",
      "2018-03-04 00:05:13,216 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 68.6%\n",
      "2018-03-04 00:05:13,219 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 66.8%\n",
      "2018-03-04 00:05:13,958 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 400: 2.308102\n",
      "2018-03-04 00:05:13,960 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 70.9%\n",
      "2018-03-04 00:05:13,963 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 69.4%\n",
      "2018-03-04 00:05:14,664 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 500: 2.138870\n",
      "2018-03-04 00:05:14,666 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 72.4%\n",
      "2018-03-04 00:05:14,669 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 69.8%\n",
      "2018-03-04 00:05:15,464 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 600: 2.005989\n",
      "2018-03-04 00:05:15,467 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 73.5%\n",
      "2018-03-04 00:05:15,470 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 70.5%\n",
      "2018-03-04 00:05:16,229 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 700: 1.896823\n",
      "2018-03-04 00:05:16,232 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 74.1%\n",
      "2018-03-04 00:05:16,234 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 71.4%\n",
      "2018-03-04 00:05:16,922 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 800: 1.804624\n",
      "2018-03-04 00:05:16,924 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 74.9%\n",
      "2018-03-04 00:05:16,927 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 71.5%\n",
      "2018-03-04 00:05:17,628 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 900: 1.725043\n",
      "2018-03-04 00:05:17,630 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 75.7%\n",
      "2018-03-04 00:05:17,633 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.0%\n",
      "2018-03-04 00:05:18,344 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1000: 1.655076\n",
      "2018-03-04 00:05:18,346 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 76.1%\n",
      "2018-03-04 00:05:18,349 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.6%\n",
      "2018-03-04 00:05:19,089 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1100: 1.592681\n",
      "2018-03-04 00:05:19,091 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 76.5%\n",
      "2018-03-04 00:05:19,094 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.5%\n",
      "2018-03-04 00:05:19,822 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1200: 1.536441\n",
      "2018-03-04 00:05:19,824 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 77.0%\n",
      "2018-03-04 00:05:19,827 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.8%\n",
      "2018-03-04 00:05:20,534 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1300: 1.485356\n",
      "2018-03-04 00:05:20,536 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 77.4%\n",
      "2018-03-04 00:05:20,539 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.5%\n",
      "2018-03-04 00:05:21,257 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1400: 1.438688\n",
      "2018-03-04 00:05:21,259 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 77.7%\n",
      "2018-03-04 00:05:21,261 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.0%\n",
      "2018-03-04 00:05:21,985 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1500: 1.395854\n",
      "2018-03-04 00:05:21,987 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 78.2%\n",
      "2018-03-04 00:05:21,990 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 72.9%\n",
      "2018-03-04 00:05:22,729 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1600: 1.356368\n",
      "2018-03-04 00:05:22,731 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 78.4%\n",
      "2018-03-04 00:05:22,734 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.2%\n",
      "2018-03-04 00:05:23,446 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1700: 1.319818\n",
      "2018-03-04 00:05:23,448 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 78.7%\n",
      "2018-03-04 00:05:23,450 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.5%\n",
      "2018-03-04 00:05:24,212 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1800: 1.285854\n",
      "2018-03-04 00:05:24,214 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 79.1%\n",
      "2018-03-04 00:05:24,217 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.4%\n",
      "2018-03-04 00:05:24,960 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 1900: 1.254180\n",
      "2018-03-04 00:05:24,962 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 79.3%\n",
      "2018-03-04 00:05:24,965 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.5%\n",
      "2018-03-04 00:05:25,817 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2000: 1.224542\n",
      "2018-03-04 00:05:25,819 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 79.4%\n",
      "2018-03-04 00:05:25,823 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.5%\n",
      "2018-03-04 00:05:26,516 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2100: 1.196724\n",
      "2018-03-04 00:05:26,519 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 79.6%\n",
      "2018-03-04 00:05:26,521 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.7%\n",
      "2018-03-04 00:05:27,263 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2200: 1.170540\n",
      "2018-03-04 00:05:27,265 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 79.9%\n",
      "2018-03-04 00:05:27,267 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.8%\n",
      "2018-03-04 00:05:28,028 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2300: 1.145832\n",
      "2018-03-04 00:05:28,030 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 80.0%\n",
      "2018-03-04 00:05:28,033 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.9%\n",
      "2018-03-04 00:05:28,773 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2400: 1.122459\n",
      "2018-03-04 00:05:28,775 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 80.2%\n",
      "2018-03-04 00:05:28,777 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.9%\n",
      "2018-03-04 00:05:29,513 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2500: 1.100298\n",
      "2018-03-04 00:05:29,515 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 80.4%\n",
      "2018-03-04 00:05:29,517 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.9%\n",
      "2018-03-04 00:05:30,199 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2600: 1.079237\n",
      "2018-03-04 00:05:30,200 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 80.7%\n",
      "2018-03-04 00:05:30,203 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 73.9%\n",
      "2018-03-04 00:05:30,887 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2700: 1.059178\n",
      "2018-03-04 00:05:30,889 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 80.9%\n",
      "2018-03-04 00:05:30,892 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.1%\n",
      "2018-03-04 00:05:31,586 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2800: 1.040031\n",
      "2018-03-04 00:05:31,588 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.1%\n",
      "2018-03-04 00:05:31,591 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.2%\n",
      "2018-03-04 00:05:32,298 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 2900: 1.021721\n",
      "2018-03-04 00:05:32,301 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.2%\n",
      "2018-03-04 00:05:32,303 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.3%\n",
      "2018-03-04 00:05:33,013 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3000: 1.004177\n",
      "2018-03-04 00:05:33,014 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.3%\n",
      "2018-03-04 00:05:33,017 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:05:33,714 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3100: 0.987341\n",
      "2018-03-04 00:05:33,716 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.4%\n",
      "2018-03-04 00:05:33,719 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:34,439 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3200: 0.971161\n",
      "2018-03-04 00:05:34,441 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.6%\n",
      "2018-03-04 00:05:34,444 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:35,139 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3300: 0.955588\n",
      "2018-03-04 00:05:35,141 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.7%\n",
      "2018-03-04 00:05:35,143 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:35,907 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3400: 0.940582\n",
      "2018-03-04 00:05:35,909 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 81.8%\n",
      "2018-03-04 00:05:35,912 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:36,611 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3500: 0.926107\n",
      "2018-03-04 00:05:36,613 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.0%\n",
      "2018-03-04 00:05:36,616 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:37,335 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3600: 0.912130\n",
      "2018-03-04 00:05:37,338 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.1%\n",
      "2018-03-04 00:05:37,340 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:38,022 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3700: 0.898620\n",
      "2018-03-04 00:05:38,025 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.2%\n",
      "2018-03-04 00:05:38,027 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:38,740 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3800: 0.885552\n",
      "2018-03-04 00:05:38,742 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.2%\n",
      "2018-03-04 00:05:38,745 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:39,464 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 3900: 0.872901\n",
      "2018-03-04 00:05:39,466 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.5%\n",
      "2018-03-04 00:05:39,468 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:40,166 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4000: 0.860644\n",
      "2018-03-04 00:05:40,168 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.6%\n",
      "2018-03-04 00:05:40,170 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:40,893 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4100: 0.848763\n",
      "2018-03-04 00:05:40,895 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.7%\n",
      "2018-03-04 00:05:40,898 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:41,567 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4200: 0.837238\n",
      "2018-03-04 00:05:41,569 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.8%\n",
      "2018-03-04 00:05:41,572 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:42,283 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4300: 0.826052\n",
      "2018-03-04 00:05:42,285 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 82.9%\n",
      "2018-03-04 00:05:42,287 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:42,996 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4400: 0.815190\n",
      "2018-03-04 00:05:42,998 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.0%\n",
      "2018-03-04 00:05:43,001 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.4%\n",
      "2018-03-04 00:05:43,721 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4500: 0.804638\n",
      "2018-03-04 00:05:43,723 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.1%\n",
      "2018-03-04 00:05:43,726 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:44,455 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4600: 0.794381\n",
      "2018-03-04 00:05:44,457 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.1%\n",
      "2018-03-04 00:05:44,459 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:45,175 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4700: 0.784408\n",
      "2018-03-04 00:05:45,177 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.3%\n",
      "2018-03-04 00:05:45,180 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:45,878 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4800: 0.774707\n",
      "2018-03-04 00:05:45,880 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.5%\n",
      "2018-03-04 00:05:45,882 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:46,616 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 4900: 0.765266\n",
      "2018-03-04 00:05:46,618 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.5%\n",
      "2018-03-04 00:05:46,624 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:47,352 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5000: 0.756076\n",
      "2018-03-04 00:05:47,354 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.6%\n",
      "2018-03-04 00:05:47,357 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:48,074 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5100: 0.747125\n",
      "2018-03-04 00:05:48,076 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.7%\n",
      "2018-03-04 00:05:48,079 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:48,790 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5200: 0.738407\n",
      "2018-03-04 00:05:48,792 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.9%\n",
      "2018-03-04 00:05:48,794 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:49,506 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5300: 0.729910\n",
      "2018-03-04 00:05:49,508 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 83.9%\n",
      "2018-03-04 00:05:49,510 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:50,240 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5400: 0.721628\n",
      "2018-03-04 00:05:50,242 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.1%\n",
      "2018-03-04 00:05:50,244 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:50,952 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5500: 0.713552\n",
      "2018-03-04 00:05:50,954 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.2%\n",
      "2018-03-04 00:05:50,957 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:05:51,679 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5600: 0.705676\n",
      "2018-03-04 00:05:51,681 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.2%\n",
      "2018-03-04 00:05:51,685 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:52,401 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5700: 0.697990\n",
      "2018-03-04 00:05:52,404 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.4%\n",
      "2018-03-04 00:05:52,407 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:53,122 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5800: 0.690491\n",
      "2018-03-04 00:05:53,124 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.5%\n",
      "2018-03-04 00:05:53,127 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:05:53,836 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 5900: 0.683170\n",
      "2018-03-04 00:05:53,839 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.7%\n",
      "2018-03-04 00:05:53,841 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:05:54,547 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6000: 0.676021\n",
      "2018-03-04 00:05:54,549 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.8%\n",
      "2018-03-04 00:05:54,552 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:55,280 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6100: 0.669040\n",
      "2018-03-04 00:05:55,282 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 84.9%\n",
      "2018-03-04 00:05:55,285 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:56,003 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6200: 0.662219\n",
      "2018-03-04 00:05:56,005 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.0%\n",
      "2018-03-04 00:05:56,008 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:05:56,728 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6300: 0.655554\n",
      "2018-03-04 00:05:56,730 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.1%\n",
      "2018-03-04 00:05:56,732 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:57,471 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6400: 0.649040\n",
      "2018-03-04 00:05:57,474 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.2%\n",
      "2018-03-04 00:05:57,477 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:58,215 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6500: 0.642673\n",
      "2018-03-04 00:05:58,217 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.3%\n",
      "2018-03-04 00:05:58,219 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:05:58,978 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6600: 0.636445\n",
      "2018-03-04 00:05:58,980 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.4%\n",
      "2018-03-04 00:05:58,983 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:05:59,725 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6700: 0.630355\n",
      "2018-03-04 00:05:59,727 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.6%\n",
      "2018-03-04 00:05:59,729 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:00,477 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6800: 0.624398\n",
      "2018-03-04 00:06:00,479 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.7%\n",
      "2018-03-04 00:06:00,481 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:01,233 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 6900: 0.618569\n",
      "2018-03-04 00:06:01,235 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.8%\n",
      "2018-03-04 00:06:01,238 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:01,915 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7000: 0.612863\n",
      "2018-03-04 00:06:01,918 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.8%\n",
      "2018-03-04 00:06:01,920 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:02,596 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7100: 0.607279\n",
      "2018-03-04 00:06:02,598 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 85.9%\n",
      "2018-03-04 00:06:02,601 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:03,302 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7200: 0.601811\n",
      "2018-03-04 00:06:03,304 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.0%\n",
      "2018-03-04 00:06:03,306 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:04,044 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7300: 0.596456\n",
      "2018-03-04 00:06:04,047 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.1%\n",
      "2018-03-04 00:06:04,049 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.4%\n",
      "2018-03-04 00:06:04,836 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7400: 0.591210\n",
      "2018-03-04 00:06:04,838 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.2%\n",
      "2018-03-04 00:06:04,840 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.5%\n",
      "2018-03-04 00:06:05,591 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7500: 0.586071\n",
      "2018-03-04 00:06:05,593 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.3%\n",
      "2018-03-04 00:06:05,595 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:06,292 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7600: 0.581036\n",
      "2018-03-04 00:06:06,294 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.4%\n",
      "2018-03-04 00:06:06,296 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:07,019 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7700: 0.576101\n",
      "2018-03-04 00:06:07,021 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.5%\n",
      "2018-03-04 00:06:07,024 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:07,725 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7800: 0.571262\n",
      "2018-03-04 00:06:07,727 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.6%\n",
      "2018-03-04 00:06:07,729 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:08,421 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 7900: 0.566518\n",
      "2018-03-04 00:06:08,423 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.8%\n",
      "2018-03-04 00:06:08,425 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:09,116 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8000: 0.561866\n",
      "2018-03-04 00:06:09,118 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 86.9%\n",
      "2018-03-04 00:06:09,120 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:09,827 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8100: 0.557303\n",
      "2018-03-04 00:06:09,829 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.0%\n",
      "2018-03-04 00:06:09,831 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:10,520 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8200: 0.552826\n",
      "2018-03-04 00:06:10,522 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.0%\n",
      "2018-03-04 00:06:10,525 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:11,287 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8300: 0.548433\n",
      "2018-03-04 00:06:11,289 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.1%\n",
      "2018-03-04 00:06:11,292 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:12,066 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8400: 0.544122\n",
      "2018-03-04 00:06:12,068 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.1%\n",
      "2018-03-04 00:06:12,071 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:12,779 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8500: 0.539890\n",
      "2018-03-04 00:06:12,781 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.2%\n",
      "2018-03-04 00:06:12,783 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:13,474 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8600: 0.535735\n",
      "2018-03-04 00:06:13,476 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.3%\n",
      "2018-03-04 00:06:13,479 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:14,177 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8700: 0.531654\n",
      "2018-03-04 00:06:14,179 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.5%\n",
      "2018-03-04 00:06:14,182 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:14,862 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8800: 0.527647\n",
      "2018-03-04 00:06:14,864 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.5%\n",
      "2018-03-04 00:06:14,867 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:15,540 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 8900: 0.523710\n",
      "2018-03-04 00:06:15,542 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.7%\n",
      "2018-03-04 00:06:15,544 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:16,263 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9000: 0.519841\n",
      "2018-03-04 00:06:16,265 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 87.8%\n",
      "2018-03-04 00:06:16,268 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:16,993 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9100: 0.516039\n",
      "2018-03-04 00:06:16,995 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.0%\n",
      "2018-03-04 00:06:16,998 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:17,702 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9200: 0.512301\n",
      "2018-03-04 00:06:17,704 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.0%\n",
      "2018-03-04 00:06:17,707 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:18,387 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9300: 0.508627\n",
      "2018-03-04 00:06:18,389 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.1%\n",
      "2018-03-04 00:06:18,392 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:19,060 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9400: 0.505013\n",
      "2018-03-04 00:06:19,062 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.1%\n",
      "2018-03-04 00:06:19,065 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:19,735 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9500: 0.501459\n",
      "2018-03-04 00:06:19,737 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.2%\n",
      "2018-03-04 00:06:19,739 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:20,396 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9600: 0.497963\n",
      "2018-03-04 00:06:20,398 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.2%\n",
      "2018-03-04 00:06:20,400 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:21,090 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9700: 0.494523\n",
      "2018-03-04 00:06:21,092 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.3%\n",
      "2018-03-04 00:06:21,095 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:21,821 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9800: 0.491137\n",
      "2018-03-04 00:06:21,823 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.3%\n",
      "2018-03-04 00:06:21,826 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.3%\n",
      "2018-03-04 00:06:22,525 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 9900: 0.487804\n",
      "2018-03-04 00:06:22,527 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.3%\n",
      "2018-03-04 00:06:22,530 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:23,248 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10000: 0.484523\n",
      "2018-03-04 00:06:23,250 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.3%\n",
      "2018-03-04 00:06:23,252 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:23,996 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10100: 0.481292\n",
      "2018-03-04 00:06:23,998 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.4%\n",
      "2018-03-04 00:06:24,001 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:24,743 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10200: 0.478110\n",
      "2018-03-04 00:06:24,745 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.4%\n",
      "2018-03-04 00:06:24,748 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:25,429 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10300: 0.474976\n",
      "2018-03-04 00:06:25,431 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.5%\n",
      "2018-03-04 00:06:25,434 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:26,134 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10400: 0.471888\n",
      "2018-03-04 00:06:26,137 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.6%\n",
      "2018-03-04 00:06:26,139 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:26,824 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10500: 0.468846\n",
      "2018-03-04 00:06:26,826 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.6%\n",
      "2018-03-04 00:06:26,829 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:27,477 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10600: 0.465848\n",
      "2018-03-04 00:06:27,479 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.6%\n",
      "2018-03-04 00:06:27,482 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:28,145 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10700: 0.462893\n",
      "2018-03-04 00:06:28,147 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.7%\n",
      "2018-03-04 00:06:28,152 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:28,835 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10800: 0.459980\n",
      "2018-03-04 00:06:28,837 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.8%\n",
      "2018-03-04 00:06:28,839 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:29,507 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 10900: 0.457108\n",
      "2018-03-04 00:06:29,509 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 88.9%\n",
      "2018-03-04 00:06:29,511 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:30,231 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11000: 0.454277\n",
      "2018-03-04 00:06:30,233 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.0%\n",
      "2018-03-04 00:06:30,235 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:30,910 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11100: 0.451485\n",
      "2018-03-04 00:06:30,912 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.1%\n",
      "2018-03-04 00:06:30,915 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:31,580 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11200: 0.448731\n",
      "2018-03-04 00:06:31,582 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.2%\n",
      "2018-03-04 00:06:31,584 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:32,270 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11300: 0.446015\n",
      "2018-03-04 00:06:32,272 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.3%\n",
      "2018-03-04 00:06:32,275 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:32,981 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11400: 0.443336\n",
      "2018-03-04 00:06:32,983 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.4%\n",
      "2018-03-04 00:06:32,986 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:33,689 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11500: 0.440693\n",
      "2018-03-04 00:06:33,691 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.4%\n",
      "2018-03-04 00:06:33,694 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:34,364 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11600: 0.438085\n",
      "2018-03-04 00:06:34,366 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.5%\n",
      "2018-03-04 00:06:34,368 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:35,031 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11700: 0.435511\n",
      "2018-03-04 00:06:35,033 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.5%\n",
      "2018-03-04 00:06:35,035 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.2%\n",
      "2018-03-04 00:06:35,713 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11800: 0.432972\n",
      "2018-03-04 00:06:35,715 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.7%\n",
      "2018-03-04 00:06:35,718 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:36,413 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 11900: 0.430466\n",
      "2018-03-04 00:06:36,415 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.7%\n",
      "2018-03-04 00:06:36,417 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:37,130 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12000: 0.427992\n",
      "2018-03-04 00:06:37,132 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.8%\n",
      "2018-03-04 00:06:37,137 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.1%\n",
      "2018-03-04 00:06:37,894 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12100: 0.425550\n",
      "2018-03-04 00:06:37,897 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.9%\n",
      "2018-03-04 00:06:37,900 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:06:38,608 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12200: 0.423140\n",
      "2018-03-04 00:06:38,610 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 89.9%\n",
      "2018-03-04 00:06:38,613 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:39,311 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12300: 0.420760\n",
      "2018-03-04 00:06:39,313 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.0%\n",
      "2018-03-04 00:06:39,316 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:06:40,013 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12400: 0.418411\n",
      "2018-03-04 00:06:40,015 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.1%\n",
      "2018-03-04 00:06:40,017 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:40,748 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12500: 0.416090\n",
      "2018-03-04 00:06:40,750 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.1%\n",
      "2018-03-04 00:06:40,753 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:41,428 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12600: 0.413799\n",
      "2018-03-04 00:06:41,430 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.1%\n",
      "2018-03-04 00:06:41,433 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:06:42,119 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12700: 0.411535\n",
      "2018-03-04 00:06:42,121 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.3%\n",
      "2018-03-04 00:06:42,125 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:06:42,811 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12800: 0.409300\n",
      "2018-03-04 00:06:42,813 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.4%\n",
      "2018-03-04 00:06:42,816 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:06:43,511 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 12900: 0.407091\n",
      "2018-03-04 00:06:43,513 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.4%\n",
      "2018-03-04 00:06:43,516 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:44,212 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13000: 0.404909\n",
      "2018-03-04 00:06:44,214 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.4%\n",
      "2018-03-04 00:06:44,216 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:44,886 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13100: 0.402753\n",
      "2018-03-04 00:06:44,888 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.4%\n",
      "2018-03-04 00:06:44,890 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:45,619 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13200: 0.400623\n",
      "2018-03-04 00:06:45,621 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.5%\n",
      "2018-03-04 00:06:45,624 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:46,340 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13300: 0.398517\n",
      "2018-03-04 00:06:46,342 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.6%\n",
      "2018-03-04 00:06:46,345 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:47,019 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13400: 0.396436\n",
      "2018-03-04 00:06:47,022 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.6%\n",
      "2018-03-04 00:06:47,024 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:06:47,741 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13500: 0.394379\n",
      "2018-03-04 00:06:47,743 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.7%\n",
      "2018-03-04 00:06:47,746 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:06:48,452 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13600: 0.392346\n",
      "2018-03-04 00:06:48,454 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.7%\n",
      "2018-03-04 00:06:48,456 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:49,150 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13700: 0.390335\n",
      "2018-03-04 00:06:49,152 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.8%\n",
      "2018-03-04 00:06:49,155 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:49,832 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13800: 0.388348\n",
      "2018-03-04 00:06:49,834 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.7%\n",
      "2018-03-04 00:06:49,837 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:50,530 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 13900: 0.386382\n",
      "2018-03-04 00:06:50,532 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.8%\n",
      "2018-03-04 00:06:50,535 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:51,251 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14000: 0.384439\n",
      "2018-03-04 00:06:51,253 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.8%\n",
      "2018-03-04 00:06:51,256 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:51,936 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14100: 0.382517\n",
      "2018-03-04 00:06:51,938 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.9%\n",
      "2018-03-04 00:06:51,941 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:52,610 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14200: 0.380616\n",
      "2018-03-04 00:06:52,612 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.9%\n",
      "2018-03-04 00:06:52,614 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:53,286 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14300: 0.378736\n",
      "2018-03-04 00:06:53,288 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.9%\n",
      "2018-03-04 00:06:53,291 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.6%\n",
      "2018-03-04 00:06:53,984 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14400: 0.376876\n",
      "2018-03-04 00:06:53,986 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 90.9%\n",
      "2018-03-04 00:06:53,989 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:54,690 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14500: 0.375036\n",
      "2018-03-04 00:06:54,692 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.0%\n",
      "2018-03-04 00:06:54,695 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:55,366 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14600: 0.373216\n",
      "2018-03-04 00:06:55,368 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.0%\n",
      "2018-03-04 00:06:55,370 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:06:56,053 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14700: 0.371415\n",
      "2018-03-04 00:06:56,055 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.0%\n",
      "2018-03-04 00:06:56,058 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:56,783 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14800: 0.369633\n",
      "2018-03-04 00:06:56,785 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.1%\n",
      "2018-03-04 00:06:56,788 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:06:57,488 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 14900: 0.367870\n",
      "2018-03-04 00:06:57,490 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.2%\n",
      "2018-03-04 00:06:57,492 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:58,184 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15000: 0.366125\n",
      "2018-03-04 00:06:58,186 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.2%\n",
      "2018-03-04 00:06:58,189 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:58,902 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15100: 0.364398\n",
      "2018-03-04 00:06:58,904 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.2%\n",
      "2018-03-04 00:06:58,907 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:06:59,602 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15200: 0.362689\n",
      "2018-03-04 00:06:59,604 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.2%\n",
      "2018-03-04 00:06:59,607 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:07:00,274 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15300: 0.360997\n",
      "2018-03-04 00:07:00,276 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.3%\n",
      "2018-03-04 00:07:00,278 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:07:00,975 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15400: 0.359323\n",
      "2018-03-04 00:07:00,977 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.3%\n",
      "2018-03-04 00:07:00,979 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 75.0%\n",
      "2018-03-04 00:07:01,695 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15500: 0.357665\n",
      "2018-03-04 00:07:01,697 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.3%\n",
      "2018-03-04 00:07:01,700 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:02,396 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15600: 0.356024\n",
      "2018-03-04 00:07:02,398 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.4%\n",
      "2018-03-04 00:07:02,401 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:03,097 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15700: 0.354400\n",
      "2018-03-04 00:07:03,099 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:03,102 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:03,798 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15800: 0.352791\n",
      "2018-03-04 00:07:03,800 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:03,803 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:04,485 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 15900: 0.351198\n",
      "2018-03-04 00:07:04,487 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:04,490 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:05,164 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16000: 0.349621\n",
      "2018-03-04 00:07:05,166 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:05,168 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:05,848 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16100: 0.348059\n",
      "2018-03-04 00:07:05,850 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:05,853 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:06,540 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16200: 0.346512\n",
      "2018-03-04 00:07:06,542 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:06,544 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:07,276 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16300: 0.344980\n",
      "2018-03-04 00:07:07,279 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.5%\n",
      "2018-03-04 00:07:07,281 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:07,969 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16400: 0.343462\n",
      "2018-03-04 00:07:07,971 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.6%\n",
      "2018-03-04 00:07:07,973 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:08,681 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16500: 0.341960\n",
      "2018-03-04 00:07:08,683 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.7%\n",
      "2018-03-04 00:07:08,685 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:09,383 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16600: 0.340471\n",
      "2018-03-04 00:07:09,385 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.7%\n",
      "2018-03-04 00:07:09,388 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:10,087 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16700: 0.338996\n",
      "2018-03-04 00:07:10,089 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.8%\n",
      "2018-03-04 00:07:10,091 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:10,787 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16800: 0.337535\n",
      "2018-03-04 00:07:10,789 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.9%\n",
      "2018-03-04 00:07:10,792 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:11,484 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 16900: 0.336087\n",
      "2018-03-04 00:07:11,486 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 91.9%\n",
      "2018-03-04 00:07:11,488 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:12,177 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17000: 0.334653\n",
      "2018-03-04 00:07:12,179 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.0%\n",
      "2018-03-04 00:07:12,181 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:12,869 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17100: 0.333232\n",
      "2018-03-04 00:07:12,871 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.0%\n",
      "2018-03-04 00:07:12,874 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:13,576 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17200: 0.331824\n",
      "2018-03-04 00:07:13,578 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.1%\n",
      "2018-03-04 00:07:13,580 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:14,270 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17300: 0.330429\n",
      "2018-03-04 00:07:14,272 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.1%\n",
      "2018-03-04 00:07:14,275 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:14,970 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17400: 0.329046\n",
      "2018-03-04 00:07:14,972 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.2%\n",
      "2018-03-04 00:07:14,974 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:15,678 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17500: 0.327676\n",
      "2018-03-04 00:07:15,680 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.2%\n",
      "2018-03-04 00:07:15,683 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:16,373 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17600: 0.326318\n",
      "2018-03-04 00:07:16,375 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.2%\n",
      "2018-03-04 00:07:16,377 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:17,067 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17700: 0.324972\n",
      "2018-03-04 00:07:17,069 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.3%\n",
      "2018-03-04 00:07:17,072 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:17,745 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17800: 0.323638\n",
      "2018-03-04 00:07:17,748 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.3%\n",
      "2018-03-04 00:07:17,750 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:18,419 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 17900: 0.322315\n",
      "2018-03-04 00:07:18,421 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.3%\n",
      "2018-03-04 00:07:18,423 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:19,109 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18000: 0.321005\n",
      "2018-03-04 00:07:19,111 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.4%\n",
      "2018-03-04 00:07:19,114 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:19,833 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18100: 0.319706\n",
      "2018-03-04 00:07:19,835 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.5%\n",
      "2018-03-04 00:07:19,837 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:20,515 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18200: 0.318417\n",
      "2018-03-04 00:07:20,518 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.5%\n",
      "2018-03-04 00:07:20,521 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:21,223 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18300: 0.317140\n",
      "2018-03-04 00:07:21,225 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.5%\n",
      "2018-03-04 00:07:21,227 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:21,941 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18400: 0.315874\n",
      "2018-03-04 00:07:21,943 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.5%\n",
      "2018-03-04 00:07:21,946 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.9%\n",
      "2018-03-04 00:07:22,689 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18500: 0.314619\n",
      "2018-03-04 00:07:22,691 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.6%\n",
      "2018-03-04 00:07:22,694 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:23,426 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18600: 0.313374\n",
      "2018-03-04 00:07:23,428 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.6%\n",
      "2018-03-04 00:07:23,430 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:24,125 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18700: 0.312140\n",
      "2018-03-04 00:07:24,127 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.6%\n",
      "2018-03-04 00:07:24,129 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:24,821 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18800: 0.310917\n",
      "2018-03-04 00:07:24,824 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.7%\n",
      "2018-03-04 00:07:24,826 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:25,512 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 18900: 0.309703\n",
      "2018-03-04 00:07:25,515 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.7%\n",
      "2018-03-04 00:07:25,518 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:26,225 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19000: 0.308500\n",
      "2018-03-04 00:07:26,227 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.7%\n",
      "2018-03-04 00:07:26,229 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:26,923 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19100: 0.307306\n",
      "2018-03-04 00:07:26,925 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.7%\n",
      "2018-03-04 00:07:26,927 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.8%\n",
      "2018-03-04 00:07:27,594 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19200: 0.306123\n",
      "2018-03-04 00:07:27,596 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.8%\n",
      "2018-03-04 00:07:27,599 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:28,253 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19300: 0.304949\n",
      "2018-03-04 00:07:28,255 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.8%\n",
      "2018-03-04 00:07:28,257 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:28,950 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19400: 0.303785\n",
      "2018-03-04 00:07:28,952 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.9%\n",
      "2018-03-04 00:07:28,954 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.7%\n",
      "2018-03-04 00:07:29,630 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19500: 0.302630\n",
      "2018-03-04 00:07:29,632 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.9%\n",
      "2018-03-04 00:07:29,634 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:07:30,317 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19600: 0.301485\n",
      "2018-03-04 00:07:30,319 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 92.9%\n",
      "2018-03-04 00:07:30,321 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:07:30,998 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19700: 0.300348\n",
      "2018-03-04 00:07:31,000 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 93.0%\n",
      "2018-03-04 00:07:31,003 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:07:31,679 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19800: 0.299221\n",
      "2018-03-04 00:07:31,681 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 93.0%\n",
      "2018-03-04 00:07:31,684 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:07:32,362 - notMNIST.TF.MLR.training.GD - INFO - Loss at step 19900: 0.298103\n",
      "2018-03-04 00:07:32,364 - notMNIST.TF.MLR.training.GD - INFO - Training accuracy: 93.0%\n",
      "2018-03-04 00:07:32,366 - notMNIST.TF.MLR.training.GD - INFO - Validation accuracy: 74.5%\n",
      "2018-03-04 00:07:33,037 - notMNIST.TF.MLR.training.GD - INFO - Test accuracy: 82.5%\n",
      "2018-03-04 00:07:33,039 - notMNIST.TF.MLR.training.GD - INFO - Trained (142.305178 sec).\n",
      "2018-03-04 00:07:33,040 - notMNIST.TF.MLR.training.GD - INFO - Script execution time: 143.927022 sec.\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb\n",
    "\n",
    "import timeit\n",
    "script_start_time = timeit.default_timer()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "\n",
    "# ********** Logging settings\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('notMNIST.TF.MLR.training.GD')\n",
    "\n",
    "file_log_handler = logging.FileHandler('logfile.log')\n",
    "logger.addHandler(file_log_handler)\n",
    "\n",
    "stderr_log_handler = logging.StreamHandler()\n",
    "logger.addHandler(stderr_log_handler)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_log_handler.setFormatter(formatter)\n",
    "stderr_log_handler.setFormatter(formatter)\n",
    "\n",
    "logger.setLevel('DEBUG')\n",
    "\n",
    "def logInfo(*args):\n",
    "  logger.info(concatenate(args))\n",
    "\n",
    "def logDebug(*args):\n",
    "  logger.debug(concatenate(args))\n",
    "  \n",
    "def logError(*args):\n",
    "  logger.error(concatenate(args))\n",
    "\n",
    "def concatenate(args):\n",
    "  return ' '.join(str(v) for v in args)\n",
    "\n",
    "# ********** End of Logging settings\n",
    "\n",
    "\n",
    "\n",
    "data_root = '.' # Change me to store data elsewhere\n",
    "\n",
    "\n",
    "# ******* load  file\n",
    "\n",
    "logInfo('Loading pickle file...')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  logDebug('Training set', train_dataset.shape, train_dataset.dtype, train_labels.shape)\n",
    "  logDebug('Validation set', valid_dataset.shape, valid_dataset.dtype, valid_labels.shape)\n",
    "  logDebug('Test set', test_dataset.shape, test_dataset.dtype, test_labels.shape)\n",
    "\n",
    "logInfo('Pickle file loaded ({:f} sec).'.format(timeit.default_timer() - start_time))\n",
    "\n",
    " \n",
    "# ******* reformat\n",
    "\n",
    "logInfo('Reformating data...')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  #reshape to (same training examples quantity, 28*28). Type from float64 to float32 (less memory used)\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  # np.arange(num_labels) -> [0,1,2,3,4,5,6,7,8,9]\n",
    "  # se labels = [1 0 ...] -> labels[:,None] = [[1][0]...]\n",
    "  # np.arange(num_labels) == labels[:,None]  -> [[False True False ...][True False False ...]...] \n",
    "  # .astype(np.float32) -> [[0.0 1.0 0.0 ...][1.0 0.0 0.0 ...]...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "logDebug('Training set', train_dataset.shape, train_labels.shape)\n",
    "logDebug('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "logDebug('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "logInfo('data reformatted ({:f} sec).'.format(timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "# ******* build computation graph\n",
    "\n",
    "\n",
    "logInfo('Building computation graph...')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# With gradient descent training, much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  # weights and bias type:  <class 'tensorflow.python.ops.variables.Variable'>\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  # logits = Y = WX+b\n",
    "  # logits type:  <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  # tf.nn.softmax_cross_entropy_with_logits = D(S(Y),L) \n",
    "  # rappresenta la misura dell'entropia, degli errori di tutti i logit Y calcolati rispetto a quanto ci aspettiamo nelle label L\n",
    "  # L sono le Hot Label, cio in formato [[0.0 1.0 0.0 ...][1.0 0.0 0.0 ...]...]\n",
    "  # Y sono i Logits\n",
    "  # loss = mean(D(S(Y),L)= 1/N * Sum (D(S(Y),L)) rappresenta la media di tutti gli errori\n",
    "  # loss type:  <class 'tensorflow.python.framework.ops.Tensor'>\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  # ad ogni passo decrementa W di -loss\n",
    "  # 0.5  , il learning rate\n",
    "  # optimizer type:  <class 'tensorflow.python.framework.ops.Operation'>\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  # S(Y) per i set di training, validation e test\n",
    "  # sono quindi nel formato formato [[0.0 1.0 0.0 ...][1.0 0.0 0.0 ...]...]\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)\n",
    "\n",
    "\n",
    "logInfo('Computation graph built ({:f} sec).'.format(timeit.default_timer() - start_time)) \n",
    "\n",
    "# ******* training\n",
    "\n",
    "\n",
    "logInfo('Training...')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "num_steps = 20000\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  # percentuale del rapporto tra \n",
    "  # il numero di predictions corrette \n",
    "  # e il numero totale di predictions per il caso in questione (train, valid o test)\n",
    "\n",
    "  # np.argmax(x,1) di una matrice x, ritorna l'indice delle colonne (axis=1) con valore massimo\n",
    "  # le label sono in hot encoding, per come  fatto funzionerebbe anche se non lo fossero\n",
    "  # le predictions dopo softmax tendono ad avere valori come [  4.26267942e-14   1.02976919e-15   9.33372438e-01 ...,   6.24717325e-02]\n",
    "  # np.argmax(predictions, 1) dar per [[0.0 0.0 1.0 ...][1.0 0.0 0.0 ...]...] -> [2 0 ...]\n",
    "  # idem np.argmax(labels, 1)\n",
    "  # np.argmax(predictions, 1) == np.argmax(labels, 1)  un array di booleani e np.sum() li converte in interi e li somma\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  logInfo('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      logInfo('Loss at step %d: %f' % (step, l))\n",
    "      logInfo('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      logInfo('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  logInfo('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "\n",
    "logInfo('Trained ({:f} sec).'.format(timeit.default_timer() - start_time))\n",
    "\n",
    "\n",
    "# Plot outputs\n",
    "\n",
    "# non si pu disegnare un grafico come quello in questo esempio dove le x corrispondono a uno scalare\n",
    "# http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n",
    "# qui d2_test_x_dataset ha shape per es. (100, 784) cio 100 (o quanti impostati) vettori con 784=28x28 valori\n",
    "\n",
    "# plt.scatter(d2_test_x_dataset, test_y_labels,  color='black')\n",
    "# plt.plot(d2_test_x_dataset, pred_y_dataset, color='blue', linewidth=3)\n",
    "\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "\n",
    "#si possono disegnare per per ogni label i valori predetti\n",
    "#quanto pi si avvicinano alla linea di inclinazione 1 tanto pi sono corretti\n",
    "\n",
    "# plt.scatter(test_y_labels, pred_y_dataset, color='blue')\n",
    "# plt.plot([0,10], [0,10], color='black', linewidth=2)\n",
    "\n",
    "# # plt.grid(True)\n",
    "# # plt.legend()\n",
    "\n",
    "# # plt.xticks(())\n",
    "# # plt.yticks(())\n",
    "\n",
    "# plt.show()\n",
    " \n",
    "\n",
    "logInfo('Script execution time: {:f} sec.'.format(timeit.default_timer() - script_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
